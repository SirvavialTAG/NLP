{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHOHVFlTRPZLJ0RhtJkIxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirvavialTAG/NLP/blob/main/NLP_lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tOKULZN_k3ZB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar_dataset = [\n",
        "    (\"The cat sleeps on the sofa.\", 1),\n",
        "    (\"She went to the library yesterday.\", 1),\n",
        "    (\"Me enjoy learning about natural language processing.\", 0),\n",
        "    (\"They are playing football in the garden.\", 1),\n",
        "    (\"He speaks English fluently.\", 1),\n",
        "    (\"We will visit the museum next week.\", 1),\n",
        "    (\"My computer needs an update.\", 1),\n",
        "    (\"Cat the sleeps sofa the on.\", 0),\n",
        "    (\"Birds fly high in the sky.\", 1),\n",
        "    (\"She goed to the library yesterday.\", 0),\n",
        "    (\"They is playing football in the garden.\", 0),\n",
        "    (\"He speak English fluently.\", 0),\n",
        "    (\"Apples are good for your health.\", 1),\n",
        "    (\"This are a very interesting book.\", 0),\n",
        "    (\"We visits the museum next week.\", 0),\n",
        "    (\"This is a very interesting book.\", 1),\n",
        "    (\"Birds flys high in the sky.\", 0),\n",
        "    (\"I enjoy learning about natural language processing.\", 1),\n",
        "    (\"My computer need an update.\", 0),\n",
        "    (\"Apples is good for your health.\", 0),\n",
        "    (\"The children are playing happily in the park.\", 1),\n",
        "    (\"We visited the museum and saw many artifacts.\", 1),\n",
        "    (\"Thinks careful before you make a decision.\", 0),\n",
        "    (\"Can you please help me with this heavy box?\", 1),\n",
        "    (\"He diligently completed all his assignments on time.\", 1),\n",
        "    (\"The old bridge look sturdy.\", 0),\n",
        "    (\"They live in a beautiful apartment overlooking the city.\", 1),\n",
        "    (\"Learn effectively require consistent practice.\", 0),\n",
        "    (\"A quick brown fox jumps over the lazy dog.\", 1),\n",
        "    (\"Did she remember to lock the door?\", 1),\n",
        "    (\"You should wear a helmet when riding a bike.\", 1),\n",
        "    (\"Water boils at 100 degrees Celsius.\", 1),\n",
        "    (\"I would appreciate your feedback on my presentation.\", 1),\n",
        "    (\"Swimming is an excellent form of exercise.\", 1),\n",
        "    (\"The old bridge looks sturdy.\", 1),\n",
        "    (\"We visit the museum and seen many artifacts.\", 0),\n",
        "    (\"This soup smells wonderful.\", 1),\n",
        "    (\"Think carefully before you make a decision.\", 1),\n",
        "    (\"The childrens is playing happy in the park.\", 0),\n",
        "    (\"Read improve your understanding of world.\", 0),\n",
        "    (\"Can you helping me with this heavy box?\", 0),\n",
        "    (\"They have known each other for many years.\", 1),\n",
        "    (\"The weather report say it might rain later today.\", 0),\n",
        "    (\"He diligent completed all his assignments on time.\", 0),\n",
        "    (\"My favorite season autumn, but my brother love summer.\", 0),\n",
        "    (\"They live beautiful apartment overlooking city.\", 0),\n",
        "    (\"You should wear a helmet when riding a bike.\", 1),\n",
        "    (\"Lazy dog the over jumps fox brown quick a.\", 0),\n",
        "    (\"The weather report says it might rain later today.\", 1),\n",
        "    (\"She did remember to locking the door?\", 0),\n",
        "    (\"You should to wear a helmet when riding a bike.\", 0),\n",
        "    (\"To learn effectively requires consistent practice.\", 1),\n",
        "    (\"Water boil at 100 degrees Celsius.\", 0),\n",
        "    (\"I would appreciates your feedback on my presentation.\", 0),\n",
        "    (\"Will they arrive before dinner?\", 1),\n",
        "    (\"Swimming are an excellent form of exercise.\", 0),\n",
        "    (\"They has know each other for many years.\", 0),\n",
        "    (\"Reading improves your understanding of the world.\", 1),\n",
        "    (\"My favorite season is autumn, but my brother loves summer.\", 1),\n",
        "    (\"This soup smell wonderful.\", 0),\n",
        "    (\"Will they arrives before dinner?\", 0),\n",
        "]"
      ],
      "metadata": {
        "id": "4VMCpoxGha93"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры\n",
        "EMBEDDING_DIM = 16\n",
        "NUM_HEADS = 2\n",
        "FFN_DIM = 64\n",
        "MAX_SEQ_LEN = 10\n",
        "VOCAB_SIZE = 250  # будет переопределено после построения словаря\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "weri6vtQhe9_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря слов из обучающего датасета\n",
        "def build_vocabulary(dataset):\n",
        "    vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    index = 2\n",
        "    for sentence, _ in dataset:\n",
        "        for word in sentence.lower().split():\n",
        "            word = word.rstrip(\".,!?;:\")\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "kF7dRViohfkH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def integer_tokenize(sentence, vocabulary):\n",
        "    tokens = []\n",
        "    for word in sentence.lower().split():\n",
        "        word = word.rstrip(\".,!?;:\")\n",
        "        if word in vocabulary:\n",
        "            tokens.append(vocabulary[word])\n",
        "        else:\n",
        "            tokens.append(vocabulary[\"<UNK>\"])\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "RHSf7olOhhuX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Приведение данных к фиксированной длине\n",
        "def pad_sequence(seq, max_len):\n",
        "    sequence = seq[:max_len]\n",
        "    return sequence + [0] * (max_len - len(sequence))"
      ],
      "metadata": {
        "id": "VVMVHvYQhjbH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset, vocabulary, max_len=MAX_SEQ_LEN):\n",
        "    X, y = [], []\n",
        "    for sentence, label in dataset:\n",
        "        tokens = integer_tokenize(sentence, vocabulary)\n",
        "        tokens = pad_sequence(tokens, max_len)\n",
        "        X.append(tokens)\n",
        "        y.append(label)\n",
        "    return np.array(X), np.array(y).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "WvhosXHxhlEX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Деление набора данных не небольшие порции\n",
        "def batch_iter(X, y, batch_size):\n",
        "    index = np.arange(len(X))\n",
        "    np.random.shuffle(index)\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch_index = index[i:i + batch_size]\n",
        "        yield X[batch_index], y[batch_index]"
      ],
      "metadata": {
        "id": "-LPZxul4hmp2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Позиционное кодированиие\n",
        "def positional_encoding(seq_len, dim_model):\n",
        "    position = np.arange(seq_len)[:, None]\n",
        "    i = np.arange(dim_model)[None, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(dim_model))\n",
        "    angle_rads = position * angle_rates\n",
        "    pos_enc = np.zeros((seq_len, dim_model))\n",
        "    pos_enc[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    pos_enc[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return pos_enc"
      ],
      "metadata": {
        "id": "QeIly5oFhoVH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Нормализация\n",
        "def layer_norm(X, epsilon=1e-6):\n",
        "    mean = np.mean(X, axis=-1, keepdims=True)\n",
        "    var = np.var(X, axis=-1, keepdims=True)\n",
        "    return (X - mean) / np.sqrt(var + epsilon)"
      ],
      "metadata": {
        "id": "OXQCl9c0hqFn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores += (mask * -1e9)\n",
        "\n",
        "    attention_weights = softmax(scores, axis=-1)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "13BgQTByhtLX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Расчёт вероятности слова\n",
        "def softmax(X, axis=-1):\n",
        "    exp_x = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "cowf8HQhhwUf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "POL3qoPlhw7H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Многоголовое внимание\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, dim_model, num_heads):\n",
        "        assert dim_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_model = dim_model\n",
        "        self.d_k = dim_model // num_heads\n",
        "\n",
        "        self.W_query = np.random.rand(dim_model, dim_model) / np.sqrt(dim_model)\n",
        "        self.W_key = np.random.rand(dim_model, dim_model) / np.sqrt(dim_model)\n",
        "        self.W_value = np.random.rand(dim_model, dim_model) / np.sqrt(dim_model)\n",
        "        self.W_output = np.random.rand(dim_model, dim_model) / np.sqrt(dim_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch, sequence, dim_model = x.shape\n",
        "        x = x.reshape(batch, sequence, self.num_heads, self.d_k)\n",
        "        return x.transpose(0, 2, 1, 3)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch, num_heads, sequence, d_k = x.shape\n",
        "        x = x.transpose(0, 2, 1, 3)\n",
        "        return x.reshape(batch, sequence, num_heads * d_k)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        Q = np.matmul(x, self.W_query)\n",
        "        K = np.matmul(x, self.W_key)\n",
        "        V = np.matmul(x, self.W_value)\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "        attention, _ = scaled_dot_product_attention(Q, K, V)\n",
        "        attention = self.combine_heads(attention)\n",
        "        output = np.matmul(attention, self.W_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "oy2FpIIfhyUn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward Network\n",
        "class FeedForward:\n",
        "    def __init__(self, dim_model, ffn_dim):\n",
        "        self.W1 = np.random.rand(dim_model, ffn_dim) / np.sqrt(dim_model)\n",
        "        self.b1 = np.zeros((1, ffn_dim))\n",
        "        self.W2 = np.random.rand(ffn_dim, dim_model) / np.sqrt(ffn_dim)\n",
        "        self.b2 = np.zeros((1, dim_model))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = np.matmul(x, self.W1) + self.b1\n",
        "        h = np.maximum(0, h)\n",
        "        output = np.matmul(h, self.W2) + self.b2\n",
        "        return output"
      ],
      "metadata": {
        "id": "01VQplPfh05_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "class Encoder:\n",
        "    def __init__(self, d_model, num_heads, ffn_dim):\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, ffn_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        attn_out = self.mha(x)\n",
        "        x = layer_norm(x + attn_out)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = layer_norm(x + ffn_out)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LFSveFswh23H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Классификатор\n",
        "class Classifier:\n",
        "    def __init__(self, d_model):\n",
        "        self.W = np.random.rand(d_model, 1) / np.sqrt(d_model)\n",
        "        self.b = np.zeros((1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        pooled = np.mean(x, axis=1)\n",
        "        logits = np.matmul(pooled, self.W) + self.b\n",
        "        return sigmoid(logits)"
      ],
      "metadata": {
        "id": "KBmWeQCGh45f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель GPT\n",
        "class GPT:\n",
        "    def __init__(self, vocabulary_size, dim_model, num_heads, ffn_dim, max_seq_len):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.dim_model = dim_model\n",
        "        self.embedding = np.random.rand(vocabulary_size, dim_model) / np.sqrt(vocabulary_size)\n",
        "        self.pos_encoding = positional_encoding(max_seq_len, dim_model)\n",
        "        self.encoder = Encoder(dim_model, num_heads, ffn_dim)\n",
        "        self.classifier = Classifier(dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding[x] + self.pos_encoding[None, :, :]\n",
        "        x = self.encoder(x)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\n",
        "    def parameters(self):\n",
        "        params = [self.embedding, self.encoder.mha.W_query, self.encoder.mha.W_key, self.encoder.mha.W_value, self.encoder.mha.W_output,\n",
        "                  self.encoder.ffn.W1, self.encoder.ffn.b1, self.encoder.ffn.W2, self.encoder.ffn.b2,\n",
        "                  self.classifier.W, self.classifier.b]\n",
        "        return params"
      ],
      "metadata": {
        "id": "4JXOK8UFh6b_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_pred, y_true):\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
      ],
      "metadata": {
        "id": "EqTDwKaeh8xH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X, y, epochs=EPOCHS, lr=LEARNING_RATE):\n",
        "    for epoch in range(epochs):\n",
        "        losses = []\n",
        "        for X_batch, y_batch in batch_iter(X, y, BATCH_SIZE):\n",
        "            # Прямой проход\n",
        "            y_pred = model.forward(X_batch)\n",
        "            loss = binary_cross_entropy(y_pred, y_batch)\n",
        "            losses.append(loss)\n",
        "            # Градиенты (численно, для простоты)\n",
        "            grads = numerical_gradients(model, X_batch, y_batch)\n",
        "            # Обновление параметров\n",
        "            for param, grad in zip(model.parameters(), grads):\n",
        "                param -= lr * grad\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.mean(losses):.4f}\")"
      ],
      "metadata": {
        "id": "WhEzJBaJh-p3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradients(model, X, y, eps=1e-4):\n",
        "    grads = []\n",
        "    for param in model.parameters():\n",
        "        grad = np.zeros_like(param)\n",
        "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            idx = it.multi_index\n",
        "            orig = param[idx]\n",
        "            param[idx] = orig + eps\n",
        "            loss1 = binary_cross_entropy(model.forward(X), y)\n",
        "            param[idx] = orig - eps\n",
        "            loss2 = binary_cross_entropy(model.forward(X), y)\n",
        "            grad[idx] = (loss1 - loss2) / (2 * eps)\n",
        "            param[idx] = orig\n",
        "            it.iternext()\n",
        "        grads.append(grad)\n",
        "    return grads"
      ],
      "metadata": {
        "id": "0XJnHqS2iA-P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, sentence, vocab):\n",
        "    tokens = integer_tokenize(sentence, vocab)\n",
        "    tokens = pad_sequence(tokens, MAX_SEQ_LEN)\n",
        "    X = np.array([tokens])\n",
        "    y_pred = model.forward(X)\n",
        "    return float(y_pred[0,0])"
      ],
      "metadata": {
        "id": "UJhj7baDiC4H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования\n",
        "vocabulary = build_vocabulary(grammar_dataset)\n",
        "X, y = prepare_data(grammar_dataset, vocabulary)\n",
        "modelGPT = GPT(len(vocabulary), EMBEDDING_DIM, NUM_HEADS, FFN_DIM, MAX_SEQ_LEN)\n",
        "train(modelGPT, X, y, epochs=EPOCHS, lr=LEARNING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLM_r2rgiEWv",
        "outputId": "ad47a714-3d0f-43f1-c7fa-a0e2bb854525"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6998\n",
            "Epoch 2/20, Loss: 0.6997\n",
            "Epoch 3/20, Loss: 0.7042\n",
            "Epoch 4/20, Loss: 0.6983\n",
            "Epoch 5/20, Loss: 0.6992\n",
            "Epoch 6/20, Loss: 0.6977\n",
            "Epoch 7/20, Loss: 0.6975\n",
            "Epoch 8/20, Loss: 0.6963\n",
            "Epoch 9/20, Loss: 0.7007\n",
            "Epoch 10/20, Loss: 0.6991\n",
            "Epoch 11/20, Loss: 0.6984\n",
            "Epoch 12/20, Loss: 0.6979\n",
            "Epoch 13/20, Loss: 0.6988\n",
            "Epoch 14/20, Loss: 0.6967\n",
            "Epoch 15/20, Loss: 0.6972\n",
            "Epoch 16/20, Loss: 0.6981\n",
            "Epoch 17/20, Loss: 0.6963\n",
            "Epoch 18/20, Loss: 0.6942\n",
            "Epoch 19/20, Loss: 0.6965\n",
            "Epoch 20/20, Loss: 0.7004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The childen sleep on the sofa:\", predict(modelGPT, \"The childen sleep on the sofa.\", vocabulary))\n",
        "print(\"This soup smell wonderful:\", predict(modelGPT, \"This soup smell wonderful.\", vocabulary))"
      ],
      "metadata": {
        "id": "9ka1XIdan-UP",
        "outputId": "43448af8-a280-4d3a-8a81-7c183c03f04f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The childen sleep on the sofa: 0.49832744895846887\n",
            "This soup smell wonderful: 0.4979063430868901\n"
          ]
        }
      ]
    }
  ]
}