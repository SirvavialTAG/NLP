{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzq7TyGxHDk4ZJj52SrnrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirvavialTAG/NLP/blob/main/NLP_lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kDMXr1MrgAw",
        "outputId": "92e32787-6eef-4702-97d9-e27a53a1dd8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3 nltk langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "import pymorphy3\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "from math import log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN7Lg5MNsSi0",
        "outputId": "29c1e9a1-1d6c-46fd-8803-0ee5323788fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_texts = [\n",
        "    \"About two years ago, in 1808, after returning to St. Petersburg from his \"\n",
        "    \"trip to the estates, Pierre unwittingly became the head of St. Petersburg \"\n",
        "    \"Freemasonry.\",\n",
        "    \"He set up mess halls and funeral lodges, recruited new members, took care \"\n",
        "    \"of connecting the various lodges and acquiring authentic acts.\",\n",
        "    \"He gave his money for the construction of temples and \"\n",
        "    \"replenished, as much as he could, the collection of alms, for which most \"\n",
        "    \"of the members were stingy and careless.\",\n",
        "    \"He supported the poor house set \"\n",
        "    \"up by the order in St. Petersburg almost alone at his own expense.\"\n",
        "]\n",
        "\n",
        "russian_texts = [\n",
        "    \"Года два тому назад, в 1808 году, вернувшись в Петербург из своей поездки \"\n",
        "    \"по имениям, Пьер невольно стал во главе петербургского масонства.\",\n",
        "    \"Он устроивал столовые и надгробные ложи, вербовал новых членов, заботился \"\n",
        "    \"о соединении различных лож и о приобретении подлинных актов.\",\n",
        "    \"Он давал свои деньги на устройство храмин и пополнял, насколько мог, \"\n",
        "    \"сборы милостыни, на которые большинство членов были скупы и неаккуратны.\",\n",
        "    \"Он почти один на свои средства поддерживал дом бедных, устроенный орденом \"\n",
        "    \"в Петербурге.\"\n",
        "]"
      ],
      "metadata": {
        "id": "iNSk1uH5sYSb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_delete_stopwords(text: str, language: str) -> list[str]:\n",
        "    all_tokens = word_tokenize(text)\n",
        "    word_tokens = [token.lower() for token in all_tokens if token not in punctuation]\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "pMfBUqQ9oORi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_russian_text(texts: list[str]) -> list[list[str]]:\n",
        "    result_texts = []\n",
        "    for text in texts:\n",
        "        filtered_tokens = tokenize_and_delete_stopwords(text, 'russian')\n",
        "        morph = pymorphy3.MorphAnalyzer()\n",
        "        lemmas = [morph.parse(token)[0].normal_form for token in filtered_tokens]\n",
        "        result_texts.append(lemmas)\n",
        "\n",
        "    return result_texts"
      ],
      "metadata": {
        "id": "kfCnZC9GsGeS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_english_text(texts: list[str]) -> list[list[str]]:\n",
        "    result_texts = []\n",
        "    for text in texts:\n",
        "        filtered_tokens = tokenize_and_delete_stopwords(text, 'english')\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        tagged_words = nltk.pos_tag(filtered_tokens)\n",
        "        tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'V': wordnet.VERB,\n",
        "        'N': wordnet.NOUN,\n",
        "        'R': wordnet.ADV\n",
        "        }\n",
        "        lemmas = [lemmatizer.lemmatize(word, pos=tag_dict.get(tag[0], wordnet.NOUN)) for word, tag in tagged_words]\n",
        "        result_texts.append(lemmas)\n",
        "\n",
        "    return result_texts"
      ],
      "metadata": {
        "id": "IxzsNs_fsG-J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_english_texts = preprocessing_english_text(english_texts)\n",
        "print(result_english_texts)\n",
        "\n",
        "result_russian_texts = preprocessing_russian_text(russian_texts)\n",
        "print(result_russian_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssRvwDrLsMhB",
        "outputId": "0f5c8f4a-760d-4378-93b5-663e40b94467"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['two', 'year', 'ago', '1808', 'return', 'st.', 'petersburg', 'trip', 'estates', 'pierre', 'unwittingly', 'become', 'head', 'st.', 'petersburg', 'freemasonry'], ['set', 'mess', 'halls', 'funeral', 'lodge', 'recruit', 'new', 'member', 'take', 'care', 'connect', 'various', 'lodge', 'acquire', 'authentic', 'act'], ['give', 'money', 'construction', 'temples', 'replenish', 'much', 'could', 'collection', 'alms', 'member', 'stingy', 'careless'], ['support', 'poor', 'house', 'set', 'order', 'st.', 'petersburg', 'almost', 'alone', 'expense']]\n",
            "[['год', 'тот', 'назад', '1808', 'год', 'вернуться', 'петербург', 'свой', 'поездка', 'имение', 'пьер', 'невольно', 'стать', 'глава', 'петербургский', 'масонство'], ['устроивать', 'столовая', 'надгробный', 'ложа', 'вербовать', 'новый', 'член', 'заботиться', 'соединение', 'различный', 'ложа', 'приобретение', 'подлинный', 'акт'], ['давать', 'свой', 'деньга', 'устройство', 'храмина', 'пополнять', 'насколько', 'мочь', 'сбор', 'милостыня', 'который', 'большинство', 'член', 'скупой', 'неаккуратный'], ['свой', 'средство', 'поддерживать', 'дом', 'бедный', 'устроенный', 'орден', 'петербург']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(processed_texts: list[str]) -> dict[str]:\n",
        "    unique_words = {}\n",
        "    for text in processed_texts:\n",
        "        for word in text:\n",
        "            if word not in unique_words:\n",
        "                unique_words[word] = 1\n",
        "            else:\n",
        "                unique_words[word] += 1\n",
        "\n",
        "    return dict(sorted(unique_words.items()))"
      ],
      "metadata": {
        "id": "1a8uK4GQtmKs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus_word_dict = make_dict(result_russian_texts)\n",
        "print(rus_word_dict)\n",
        "\n",
        "eng_word_dict = make_dict(result_english_texts)\n",
        "print(eng_word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yQCVKz_cWqV",
        "outputId": "12217dc0-ddbe-4f8d-d103-d1ce6b1f3da9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1808': 1, 'акт': 1, 'бедный': 1, 'большинство': 1, 'вербовать': 1, 'вернуться': 1, 'глава': 1, 'год': 2, 'давать': 1, 'деньга': 1, 'дом': 1, 'заботиться': 1, 'имение': 1, 'который': 1, 'ложа': 2, 'масонство': 1, 'милостыня': 1, 'мочь': 1, 'надгробный': 1, 'назад': 1, 'насколько': 1, 'неаккуратный': 1, 'невольно': 1, 'новый': 1, 'орден': 1, 'петербург': 2, 'петербургский': 1, 'поддерживать': 1, 'подлинный': 1, 'поездка': 1, 'пополнять': 1, 'приобретение': 1, 'пьер': 1, 'различный': 1, 'сбор': 1, 'свой': 3, 'скупой': 1, 'соединение': 1, 'средство': 1, 'стать': 1, 'столовая': 1, 'тот': 1, 'устроенный': 1, 'устроивать': 1, 'устройство': 1, 'храмина': 1, 'член': 2}\n",
            "{'1808': 1, 'acquire': 1, 'act': 1, 'ago': 1, 'almost': 1, 'alms': 1, 'alone': 1, 'authentic': 1, 'become': 1, 'care': 1, 'careless': 1, 'collection': 1, 'connect': 1, 'construction': 1, 'could': 1, 'estates': 1, 'expense': 1, 'freemasonry': 1, 'funeral': 1, 'give': 1, 'halls': 1, 'head': 1, 'house': 1, 'lodge': 2, 'member': 2, 'mess': 1, 'money': 1, 'much': 1, 'new': 1, 'order': 1, 'petersburg': 3, 'pierre': 1, 'poor': 1, 'recruit': 1, 'replenish': 1, 'return': 1, 'set': 2, 'st.': 3, 'stingy': 1, 'support': 1, 'take': 1, 'temples': 1, 'trip': 1, 'two': 1, 'unwittingly': 1, 'various': 1, 'year': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "def count_num_words(texts: list[str], word_dict: dict[str, int]) -> list[list[int]]:\n",
        "    matrix_words = np.zeros((len(texts), len(word_dict)), dtype=int)\n",
        "    sorted_words = sorted(word_dict.keys())\n",
        "    for i, text in enumerate(texts):\n",
        "        for word in text:\n",
        "            col_index = sorted_words.index(word)\n",
        "            matrix_words[i][col_index] += 1\n",
        "\n",
        "    return matrix_words"
      ],
      "metadata": {
        "id": "C6guyx9UzjiT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus_words_matrix = count_num_words(result_russian_texts, rus_word_dict)\n",
        "print(\"Русский текст:\\n\", rus_words_matrix)\n",
        "\n",
        "eng_words_matrix = count_num_words(result_english_texts, eng_word_dict)\n",
        "print(\"\\nАнглийский текст:\\n\", eng_words_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF53D_UKeBnl",
        "outputId": "7664df3f-1729-457d-ec90-c2924fad86b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русский текст:\n",
            " [[1 0 0 0 0 1 1 2 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            "  0 0 0 1 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 1 0 0 0 0 0 0 1 0 0 2 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0\n",
            "  0 1 0 0 1 0 0 1 0 0 1]\n",
            " [0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1\n",
            "  1 0 0 0 0 0 0 0 1 1 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1\n",
            "  0 0 1 0 0 0 1 0 0 0 0]]\n",
            "\n",
            "Английский текст:\n",
            " [[1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 1 0 0 0 1\n",
            "  0 2 0 0 0 0 1 1 1 0 1]\n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 2 1 1 0 0 1 0 0 0 0 1 0 0\n",
            "  1 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0\n",
            "  0 0 1 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            "  1 1 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "def calculate_tf_idf(matrix: list[list[int]]) -> list[list[int]]:\n",
        "    # TF\n",
        "    tf = matrix / matrix.sum(axis=1, keepdims=True)\n",
        "    # IDF\n",
        "    N = matrix.shape[0]\n",
        "    M = (matrix > 0).sum(axis=0)\n",
        "    idf = np.log(N / (1 + M))\n",
        "    # TF-IDF\n",
        "    tf_idf_matrix = tf * idf\n",
        "\n",
        "    return tf_idf_matrix"
      ],
      "metadata": {
        "id": "Y-DUj7fv5DHD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_rus_matrix = calculate_tf_idf(rus_words_matrix)\n",
        "print(\"Русский текст:\\n\", result_rus_matrix)\n",
        "\n",
        "result_eng_matrix = calculate_tf_idf(eng_words_matrix)\n",
        "print(\"\\nАнглийский текст:\\n\", result_eng_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL3LbKMZhT_2",
        "outputId": "c0efc5e7-5fbb-4c02-be9e-2e7d8e1b2635"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русский текст:\n",
            " [[0.0433217  0.         0.         0.         0.         0.0433217\n",
            "  0.0433217  0.0866434  0.         0.         0.         0.\n",
            "  0.0433217  0.         0.         0.0433217  0.         0.\n",
            "  0.         0.0433217  0.         0.         0.0433217  0.\n",
            "  0.         0.01798013 0.0433217  0.         0.         0.0433217\n",
            "  0.         0.         0.0433217  0.         0.         0.\n",
            "  0.         0.         0.         0.0433217  0.         0.0433217\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.04951051 0.         0.         0.04951051 0.\n",
            "  0.         0.         0.         0.         0.         0.04951051\n",
            "  0.         0.         0.09902103 0.         0.         0.\n",
            "  0.04951051 0.         0.         0.         0.         0.04951051\n",
            "  0.         0.         0.         0.         0.04951051 0.\n",
            "  0.         0.04951051 0.         0.04951051 0.         0.\n",
            "  0.         0.04951051 0.         0.         0.04951051 0.\n",
            "  0.         0.04951051 0.         0.         0.02054872]\n",
            " [0.         0.         0.         0.04620981 0.         0.\n",
            "  0.         0.         0.04620981 0.04620981 0.         0.\n",
            "  0.         0.04620981 0.         0.         0.04620981 0.04620981\n",
            "  0.         0.         0.04620981 0.04620981 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.04620981 0.         0.         0.         0.04620981 0.\n",
            "  0.04620981 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.04620981 0.04620981 0.0191788 ]\n",
            " [0.         0.         0.0866434  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.0866434  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.0866434  0.03596026 0.         0.0866434  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.0866434  0.         0.         0.\n",
            "  0.0866434  0.         0.         0.         0.        ]]\n",
            "\n",
            "Английский текст:\n",
            " [[0.0433217  0.         0.         0.0433217  0.         0.\n",
            "  0.         0.         0.0433217  0.         0.         0.\n",
            "  0.         0.         0.         0.0433217  0.         0.0433217\n",
            "  0.         0.         0.         0.0433217  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.03596026 0.0433217  0.         0.         0.         0.0433217\n",
            "  0.         0.03596026 0.         0.         0.         0.\n",
            "  0.0433217  0.0433217  0.0433217  0.         0.0433217 ]\n",
            " [0.         0.0433217  0.0433217  0.         0.         0.\n",
            "  0.         0.0433217  0.         0.0433217  0.         0.\n",
            "  0.0433217  0.         0.         0.         0.         0.\n",
            "  0.0433217  0.         0.0433217  0.         0.         0.0866434\n",
            "  0.01798013 0.0433217  0.         0.         0.0433217  0.\n",
            "  0.         0.         0.         0.0433217  0.         0.\n",
            "  0.01798013 0.         0.         0.         0.0433217  0.\n",
            "  0.         0.         0.         0.0433217  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.05776227\n",
            "  0.         0.         0.         0.         0.05776227 0.05776227\n",
            "  0.         0.05776227 0.05776227 0.         0.         0.\n",
            "  0.         0.05776227 0.         0.         0.         0.\n",
            "  0.02397351 0.         0.05776227 0.05776227 0.         0.\n",
            "  0.         0.         0.         0.         0.05776227 0.\n",
            "  0.         0.         0.05776227 0.         0.         0.05776227\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.06931472 0.\n",
            "  0.06931472 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.06931472 0.\n",
            "  0.         0.         0.         0.         0.06931472 0.\n",
            "  0.         0.         0.         0.         0.         0.06931472\n",
            "  0.02876821 0.         0.06931472 0.         0.         0.\n",
            "  0.02876821 0.02876821 0.         0.06931472 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}