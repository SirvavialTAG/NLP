{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN81o538tremAZqOegIMBWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirvavialTAG/NLP/blob/main/NLP_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4cPd_IhB0A",
        "outputId": "77a940a8-9b60-4b83-dd7b-412ea39d19b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3 nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "M6pH1gEPh0e2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"About two years ago, in 1808, after returning to St. Petersburg from his \"\n",
        "    \"trip to the estates, Pierre unwittingly became the head of St. Petersburg \"\n",
        "    \"Freemasonry. He set up mess halls and funeral lodges, recruited new \"\n",
        "    \"members, took care of connecting the various lodges and acquiring \"\n",
        "    \"authentic acts. He gave his money for the construction of temples and \"\n",
        "    \"replenished, as much as he could, the collection of alms, for which most \"\n",
        "    \"of the members were stingy and careless. He supported the poor house set \"\n",
        "    \"up by the order in St. Petersburg almost alone at his own expense.\"\n",
        ")\n",
        "\n",
        "text = \"Boys were playing soccer on the playground\""
      ],
      "metadata": {
        "id": "GQwMcAt5nJPG"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Лемматизация**"
      ],
      "metadata": {
        "id": "AUNSiBTka4n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'V': wordnet.VERB,\n",
        "        'N': wordnet.NOUN,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag[0], None)\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = text.split()\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "lemmatized_text = []\n",
        "for word, tag in tagged_words:\n",
        "  pos = get_wordnet_pos(tag)\n",
        "  if pos is not None:\n",
        "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "    lemmatized_text.append(lemma)\n",
        "  else:\n",
        "    lemmatized_text.append(word)\n",
        "\n",
        "joined_lemmatized_text = \" \".join(lemmatized_text)\n",
        "print(\"Лемматизация:\", joined_lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsOHMmZ_lAit",
        "outputId": "101bb9a5-df56-4e94-bc96-d79e6db3f1e2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизация: Boys be play soccer on the playground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Стемминг**"
      ],
      "metadata": {
        "id": "5T2TN187a8mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_text = [stemmer.stem(word) for word in text.split()]\n",
        "joined_stemmed_text = \" \".join(stemmed_text)\n",
        "print(\"Стемминг:\", joined_stemmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAdBAOKeuvEN",
        "outputId": "466e7f8a-9eb1-4230-849a-1124785f17e7"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стемминг: boy were play soccer on the playground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Токенизация**"
      ],
      "metadata": {
        "id": "JDknYRCabAEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> list[str]:\n",
        "  tokens = []\n",
        "  for char in text:\n",
        "    if char.isascii():\n",
        "      tokens.append(char)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "2REkoRCFw3Wl"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_1 = \"Текст для проверки работы пользовательской функции токенизации.\"\n",
        "test_text_2 = (\n",
        "          \"Text for verifying the operation of a custom tokenization function.\"\n",
        ")\n",
        "\n",
        "test_tokenize_text_1 = tokenize(test_text_1)\n",
        "test_tokenize_text_2 = tokenize(test_text_2)\n",
        "\n",
        "print(\"Тест 1: \", test_tokenize_text_1)\n",
        "print(\"Тест 2: \", tokenize(test_text_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3OZb5ojzj5u",
        "outputId": "f8e95d67-6ef8-44e7-d2e3-cd455cfb1418"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тест 1:  [' ', ' ', ' ', ' ', ' ', ' ', '.']\n",
            "Тест 2:  ['T', 'e', 'x', 't', ' ', 'f', 'o', 'r', ' ', 'v', 'e', 'r', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'o', 'p', 'e', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'a', ' ', 'c', 'u', 's', 't', 'o', 'm', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Векторизация**"
      ],
      "metadata": {
        "id": "DN5fs08ybDqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "  vectors = [ord(char) for char in tokens]\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "UUAhjtRQ5KCV"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectorize_text_1 = vectorize(test_tokenize_text_1)\n",
        "test_vectorize_text_2 = vectorize(test_tokenize_text_2)\n",
        "\n",
        "print(\"Тест 3: \", test_vectorize_text_1)\n",
        "print(\"Тест 4: \", test_vectorize_text_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwaC-jYj87mu",
        "outputId": "9029e3ea-346f-4b1a-f9fd-ba4b5c1628ef"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тест 3:  [32, 32, 32, 32, 32, 32, 46]\n",
            "Тест 4:  [84, 101, 120, 116, 32, 102, 111, 114, 32, 118, 101, 114, 105, 102, 121, 105, 110, 103, 32, 116, 104, 101, 32, 111, 112, 101, 114, 97, 116, 105, 111, 110, 32, 111, 102, 32, 97, 32, 99, 117, 115, 116, 111, 109, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 102, 117, 110, 99, 116, 105, 111, 110, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Токенизация и векторизация после лемматизации**"
      ],
      "metadata": {
        "id": "P_byNnJValac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_lemmatized_text = tokenize(joined_lemmatized_text)\n",
        "print(tokenize_lemmatized_text[:45])\n",
        "\n",
        "vectorize_lemmatized_text = vectorize(tokenize_lemmatized_text)\n",
        "print(vectorize_lemmatized_text[:45])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2g41xWVBuyd",
        "outputId": "767b1583-275b-4173-fb5b-051640178a7c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B', 'o', 'y', 's', ' ', 'b', 'e', ' ', 'p', 'l', 'a', 'y', ' ', 's', 'o', 'c', 'c', 'e', 'r', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd']\n",
            "[66, 111, 121, 115, 32, 98, 101, 32, 112, 108, 97, 121, 32, 115, 111, 99, 99, 101, 114, 32, 111, 110, 32, 116, 104, 101, 32, 112, 108, 97, 121, 103, 114, 111, 117, 110, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Токенизация и векторизация после стемминга**"
      ],
      "metadata": {
        "id": "a74aaAMmaxAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_stemmed_text = tokenize(joined_stemmed_text)\n",
        "print(tokenize_stemmed_text[:45])\n",
        "\n",
        "vectorize_stemmed_text = vectorize(tokenize_stemmed_text)\n",
        "print(vectorize_stemmed_text[:45])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oho86fuca23E",
        "outputId": "da1a2e64-e9c6-4b92-940a-bc43d0d21285"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['b', 'o', 'y', ' ', 'w', 'e', 'r', 'e', ' ', 'p', 'l', 'a', 'y', ' ', 's', 'o', 'c', 'c', 'e', 'r', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd']\n",
            "[98, 111, 121, 32, 119, 101, 114, 101, 32, 112, 108, 97, 121, 32, 115, 111, 99, 99, 101, 114, 32, 111, 110, 32, 116, 104, 101, 32, 112, 108, 97, 121, 103, 114, 111, 117, 110, 100]\n"
          ]
        }
      ]
    }
  ]
}